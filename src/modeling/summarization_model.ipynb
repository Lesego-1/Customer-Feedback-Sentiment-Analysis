{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bart_model(text):\n",
    "#     '''\n",
    "#     Uses the BART pre-trained model to summurize text.\n",
    "#     Returns the summurized text.'''\n",
    "#     # Load BART model and tokenizer\n",
    "#     model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "#     tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    \n",
    "#     inputs = tokenizer([text], max_length=75, return_tensors='tf', truncation=True) # Tokenize input text\n",
    "#     summary_ids = model.generate(inputs['input_ids'], max_length=20, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True) # Generate the summary id's\n",
    "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True) # Decode summary id's to get actual summary\n",
    "    \n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_text(dataframe, text_col):\n",
    "#     # Summurizes text and inserts summarizations into new column in the DataFrame.\n",
    "#     dataframe['Summary'] = dataframe[text_col].apply(bart_model)\n",
    "    \n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    # Returns the summarizes text\n",
    "    summarizer = pipeline(\"summarization\") # Intialize summarizer\n",
    "    \n",
    "    summary = summarizer(text, max_length=20, min_length=1, do_sample=False) # Genrate summary\n",
    "    \n",
    "    return summary[0]['summary_text'] # Get summarized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LESEGO\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
    "    The ultimate objective of NLP is to enable computers to understand, interpret, and respond to human languages in a valuable way.\n",
    "    NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
    "    When the text has been provided, the machine will utilize algorithms to extract meaning associated with every sentence and collect essential data from them.\"\"\"\n",
    "print(summarize_text(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
